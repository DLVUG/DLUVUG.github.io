import{_ as a,o as e,c as t,a as i}from"./app.04b35b91.js";const x=JSON.parse('{"title":"softmax回归","description":"softmax回归","frontmatter":{"title":"softmax回归","description":"softmax回归","date":"2022-09-30T00:00:00.000Z","tags":["张栋","ALL"]},"headers":[{"level":2,"title":"一、概念","slug":"一、概念","link":"#一、概念","children":[]},{"level":2,"title":"二、回归模型","slug":"二、回归模型","link":"#二、回归模型","children":[]},{"level":2,"title":"三、softmax函数","slug":"三、softmax函数","link":"#三、softmax函数","children":[]},{"level":2,"title":"四、损失函数","slug":"四、损失函数","link":"#四、损失函数","children":[]},{"level":2,"title":"五、优化算法","slug":"五、优化算法","link":"#五、优化算法","children":[]},{"level":2,"title":"六、模型预测与评价","slug":"六、模型预测与评价","link":"#六、模型预测与评价","children":[]}],"relativePath":"doc/softmax回归.md"}'),s={name:"doc/softmax回归.md"},o=i('<h1 id="softmax回归" tabindex="-1">softmax回归 <a class="header-anchor" href="#softmax回归" aria-hidden="true">#</a></h1><h2 id="一、概念" tabindex="-1">一、概念 <a class="header-anchor" href="#一、概念" aria-hidden="true">#</a></h2><p>Softmax回归(Softmax regression)，也称为多项（Multinomial)或多类（Multi-Class)的Logistic回归，是Logistic回归在多分类问题上的推广。</p><h2 id="二、回归模型" tabindex="-1">二、回归模型 <a class="header-anchor" href="#二、回归模型" aria-hidden="true">#</a></h2><p>Softmax回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，Softmax回归的输出值个数等于标签里的类别数。比如一共有4种特征和3种输出动物类别(狗、猫、鸡），则权重包含12个标量（带下标的w ）、偏差包含3个标量（带下标的b ），且对每个输入计算o1,o2 ,o3的输出：</p><p><img src="" alt="image-20220928145718808"></p><p>最后，再对这些输出值进行softmax函数运算。</p><p><img src="" alt="image-20220928145812103"></p><p>图用神经网络图描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出o1,o2,o3的计算都要依赖所有的输入x1,x2,x3,x4,所以softmax回归的输出层也是一个全连接层。</p><h2 id="三、softmax函数" tabindex="-1">三、softmax函数 <a class="header-anchor" href="#三、softmax函数" aria-hidden="true">#</a></h2><p>Softmax用于多分类过程中，它将多个神经元的输出(o1,o2,o3)映射到（0，1）区间内，可以看成概率来理解，从而来进行多分类。</p><p><img src="" alt="image-20220928150652285"></p><p>softmax回归对样本i分类的矢量计算表达式为:</p><p><img src="" alt="image-20220928150844264"></p><h2 id="四、损失函数" tabindex="-1">四、损失函数 <a class="header-anchor" href="#四、损失函数" aria-hidden="true">#</a></h2><p>交叉熵损失函数：</p><p><img src="https://img-blog.csdnimg.cn/20200908162036235.png" alt="img"></p><p>f(zc)是 softmax函数的输出值。yc就是样本真值。l就是最终的损失函数值了。</p><p><img src="" alt="image-20220928152753837"></p><p><img src="" alt="image-20220928152809118">显然，与真值越接近，损失函数越小，与真值相去越远 ，损失函数越大。优化过程就是不断的将与真值接近的那个概率值提升，提升，再提升，让损失函数降低，降低，再降低。</p><h2 id="五、优化算法" tabindex="-1">五、优化算法 <a class="header-anchor" href="#五、优化算法" aria-hidden="true">#</a></h2><p>梯度下降：</p><p><img src="" alt="image-20220928153049930"></p><h2 id="六、模型预测与评价" tabindex="-1">六、模型预测与评价 <a class="header-anchor" href="#六、模型预测与评价" aria-hidden="true">#</a></h2><p>预测：</p><p>在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。通常，我们把预测概率最大的类别作为输出类别。</p><p>评价：</p><p>如果它与真实类别（标签）一致，说明这次预测是正确的。一般使用准确率（accuracy）来评价模型的表现，它等于正确预测数量与总预测数量之比</p>',28),r=[o];function l(c,d,n,p,h,m){return e(),t("div",null,r)}const g=a(s,[["render",l]]);export{x as __pageData,g as default};
