---
title: Loss
description: loss
date: 2022-09-30
tags:
  - 何健飞
  - ALL
---



# classify

## distribution-based

### binary-cross-entropy

***形式***

![]()

***解释***

* 对于每一个样本，yi代表该类真实概率（只能取0或者1），h(xi)代表该类预测值（可以取0-1内的任意数）
* 当yi=1时，loss只计算前半部分，此时loss关于预测值是单减的，预测值y^越接近1，loss值越小；预测值y^越接近0，loss值越大
* 当yi=0时，loss只计算后半部分，此时loss关于预测值是单增的，预测值y^越接近1，loss值越大；预测值y^越接近0，loss值越小
* 此loss目的是让标签为1的类别其概率更接近1，标签为0的类别其概率更接近0

### multi-cross-entropy

***形式一：只考虑标签类的损失***

![]()

---



![]()

***解释***

* 样本的数量为N，类别数为K，网络在预测时会为每个样本生成K个预测值，取K个预测值中的最大值作为最终的类别
* 此时的 loss = -(0 * log0.1 + 0 * log0.5 + 1 * log0.1 + 0 * log0.3) = -(log0.1)
* 在此形式中，只有标签为1的部分才会算入损失，loss对此位置的预测结果y^是单减的，预测结果y^越接近1，loss越小
* 若使用此形式，那么分错的部分不会算入loss中，只有分对的类别会算入损失。意味着标签为0的部分在预测结果中对应的部分不会得到更新

***形式二：考虑所有标签通道上的损失***

![]()

![]()

***解释***

* 样本的数量为N，类别数为K
* 此时的loss = -(log(0.9) + log(0.5) + log(0.1) + log(0.7))
* 在此形式中，既考虑了分错的样本，也考虑了分对的样本。所以不管标签值为0的位置还是标签值为1的位置，其对应的参数都会得到更新。0更倾向于0，1更倾向于1
* 对于标签为1的位置，loss对于此位置的预测值是单减的，预测值越接近于1，loss值越小；预测值越接近于0，loss值越大，网络会使他往1的方向移动
* 对于标签为0的位置，loss对于此位置的预测值是单增的，预测值越接近于1，loss值越大；预测值越接近于0，loss值越小，网络会使他往0的方向移动

***总结***

* 形式二由于考虑了标签为0对应位置元素的损失，所以形式二的损失是大于形式一的，但对于某一个样本点在不同通道上预测的结果可能差距更大，更易于辨别。**`（有待验证）`**
* 用交叉熵计算多分类损失时常用形式一

### Weighted Cross-Entropy

***形式***

![]()

***解释***

* 通过为正样本的损失添加系数来改变正样本在整个损失中所占的比例
* 如果出现FN情况，即把正样本预测成了负样本，可以令系数大于1，此时正样本的损失所占权重变大
* 如果出现FP情况，即把负样本预测成了正样本，可以令系数小于1，此时正样本的损失所占权重变小

### Balanced Cross-Entropy

***形式***

![]()

***解释***

* 为正样本的损失和负样本的损失都添加系数，且二者系数之和为1
* 一般可以设系数 β = 1 −y/(H∗W)

### Focal Loss

***形式一：给难分样本和易分样本添加权重***

![]() 注意，公式里的pt既代表正样本也代表负样本 ![]()

***解释***

* 参数伽马是大于零的
* 当真实值 y = 1时，若预测值pt ~ 0，我们就可以把此样本看作难分样本（因为期望预测为1，但预测却为零）。此时1-pt ~ 1，不会太影响此样本对总体loss的贡献，`即不会改变难分样本loss在总loss中所占的权重`
* 当真实值 y = 1·时，若预测值pt ~ 1，我们就可以把此样本看作易分样本（因为期望预测为1，预测也接近为1）。此时1-pt ~ 0，此样本对总体loss的贡献减小，`即降低了易分样本loss在总loss中所占的比重`

***结果***

![]()

* 当伽马为零时，focal loss 和普通的CE loss 是等价的，如上图的蓝线所示
* 伽马取值大于零时，预测值越接近于0，斜率越大，意味着梯度越大；
* 预测值越接近于1，loss越小，即降低了易分样本loss在总样本loss中的比重，同时斜率越小，意味着梯度越小

***思考***

* 通过降低样本在loss中的比重真的可以到达不训练的目的吗，或者说使降低权重的那一部分loss梯度变小？`有待可视化梯度验证`
* 在多分类第一种形式中，我们并没有考虑负样本的损失，是不是代表着负样本偏多也影响不大？`有待验证`
* 这么一想，在多分类第二种形式的交叉熵计算公式中，我们是认为所有难分样本和易分样本的权重是一样的。这样对于负样本偏多的数据集来说，反倒让模型花费更多的精力去降低负样本的loss，导致正样本的损失被淹没`（并没有真正的被淹没，只是值在总体loss中占比较小，可能长时间得不到更新）`。因为尽管负样本loss和正样本loss的权重是一样的，但负样本的数量过多，相当于正样本loss在总loss中占比较小

***形式二：在形式一的基础上给多数样本和少数样本添加权重***

![]()

***解释***

* 类似于Balanced Cross-Entropy，通过给样本添加权重来调节由于正负样本数量差异过大导致的loss中正负样本所贡献loss不同的影响。给占多数的样本一个小的系数，占少数的样本一个大的系数
* 基于此形式，既可以平衡正负样本数量带来的影响，也可以平衡正负样本中难分样本loss和易分样本loss在总loss中的比重，使模型将更多的精力放在难分样本中
* 注意：正负样本中可能都存在难分的样本

***总结***

* focal loss可以在正负样本差异较大的时候可以使用

---

# regression

## bounding box

### IOU

***形式***

![]()

***解释***

* 分子是两个矩形的交集面积，分母是两个矩形并集的面积

***优点***

* 具有尺度不变性，不受矩阵大小的影响`（因为除以了并集，进行了归一化）`
* 对于相交的矩形，可以修正两个框的位置使其慢慢重合

***缺点***

* 当两个矩形不相交时，IOU=0，此时不能衡量出两个框离散的程度`（如下图1所示）`，梯度为零，框的位置得不到更新
* 当两个矩形相交时，IOU并不能反映出二者是如何相交的。尤其当相交方式不同，而IOU相同时，`如下图2所示`。

![]()

图1

![]()

图2

### GIOU

***形式***

为了改进IOU的缺陷，GIOU引入了两个矩阵外围矩形的概念，来衡量两个矩形的相交方式

![]()

***解释***

* C代表包含矩形A和B最小的外围矩形，如下图1所示
* 首先计算A和B的交并比，即IOU；再计算C - A U B 的面积 / C的面积
* 当A，B正好重合时，A U B的大小等于A或者B的大小，外围矩形C的大小等于A的面积，此时分子为零，GIOU = IOU
* 当A，B重合的区域占比很大时，A U B所在的区域在C中占的比例较大，此时C - A U B 很小，分子趋近于零
* GIOU永远都是小于IOU的

![]()

***优点***

* 可以更好的反应两个矩形的相交情况，如下图所示
* 当两个矩形不相交时，loss也不为零，可以指导框的移动

![]()

### DIOU

虽然GIOU的引入可以解决两个不相交时框如何移动的问题，但当框重叠或一个框完全在另一个框内时，GIOU退化为IOU，如下图所示。为了解决此问题，人们又提出了DIOU。

![]()

***形式***

![]()

***解释***

* 计算DIOU时还是需要先算出A和B的最小外围矩形框C，c代表C的对角线长度，b代表预测框的中心点坐标，bgt代表真实框的中心点坐标，分子代表这两个中心点的欧氏距离
* 除以c^2的原因是为了归一化，统一量纲

![]()

***优点***

* DIOU在预测框和真实框IOU为零时，仍能优化框的移动
* DIOU可以直接最小化两个框之间的距离，因此收敛的较快`（有待证实）`
* 对于包含两个框在水平方向和垂直方向上这种情况，DIoU损失可以使回归非常快，而GIoU损失几乎退化为IoU损失

### CIOU

一个好的目标框回归损失应该考虑三个重要的几何因素：**重叠面积、中心点距离、长宽比**。

GIoU：为了归一化坐标尺度，利用IoU，并初步解决IoU为零的情况。

DIoU：DIoU损失同时考虑了边界框的重叠面积和中心点距离。
然而，anchor框和目标框之间的长宽比的一致性也是极其重要的。

***形式***

![]()

***解释***

* wgt，hgt代表真实框的宽和高，w，h代表预测框的宽和高

---

# Segmentation

## region-based

### DICE

***形式一***

![]()

***形式二***

![]()

***形式三***

![]()

***举例***

![])

* A = 0.3 + 0.2 + 0.3 + 0.2 = 1.0
* B = 0 + 1 + 0 + 1 = 2
* U = A+ B = 3.0
* I = A * B = 0.3 * 0 + 0.2 * 1 + 0.3  * 0 + 0.2 * 1  = 0.4

***解释***

* 记分割预测结果为A，分割真实标签为B，U代表 A + B 的结果，I 代表 A*B 的结果
* ![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon)参数称为平滑系数，常设为1，目的是为了防止分母为零的情况
* 形式三中以元素的平方和作为U

***对一个点的函数曲线分析***

![]()

绘制曲线图如下，其中蓝色的为ce loss，橙色的为dice loss。

![]()


* 当![[公式]](https://www.zhihu.com/equation?tex=t%3D0) 时，![[公式]](https://www.zhihu.com/equation?tex=x) 在一个较大的范围内，loss的值都很大接近1。只有![[公式]](https://www.zhihu.com/equation?tex=x) 预测非常小，![[公式]](https://www.zhihu.com/equation?tex=y) 接近于0(和![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon) 量级相近)时loss才会变小，而这种情况出现的概率也较小。一般情况下，在正常范围内，预测不管为任何值，都无差别对待，loss 都统一非常大。
* 当![[公式]](https://www.zhihu.com/equation?tex=t%3D1) 时，![[公式]](https://www.zhihu.com/equation?tex=x) 在0左右较小的范围内，保持不错的特性。但随着![[公式]](https://www.zhihu.com/equation?tex=x) 远离0点，loss呈现饱和现象。

***对一个点的梯度分析***

![]()

绘图如下

![]()


## Boundary-based Loss

## Compounded Loss



长尾分布

重复训练loss高的样本，更改dataloader

语义间的loss，构建语义关系网，自然语言的loss

多模态融合loss
