---
aside: false
---



更深层次的神经网络更难训练。我们提出了一个剩余学习框架来简化网络的训练，它比以前使用的网络要深入得多。我们显式地将这些层重新表述为参考层输入的学习剩余函数，而不是学习未引用的函数。我们提供了全面的经验证据，表明这些残差网络更容易优化，并可以从相当大的深度获得准确性。在ImageNet数据集上，我们评估了深度高达152层的残余网，比VGG网[40]深8倍，但仍然具有较低的复杂度。这些残差网的集合在ImageNet测试集上的误差达到3.57%。该成果在ILSVRC 2015分类任务中获得第一名。我们还对100层和1000层的CIFAR-10进行了分析。表征的深度对于许多视觉识别任务都是至关重要的。仅仅由于我们非常深入的表示，我们获得了相对于COCO对象检测数据集28%的改进。深度残差网是我们提交给ILSVRC和COCO 2015竞赛的基础，我们还在ImageNet检测、ImageNet定位、COCO检测和COCO分割任务中获得了第一名。

<iframe src="/papers/resnet.pdf" width="100%" height="700"> </iframe>

[下载链接](/papers/resnet.pdf)